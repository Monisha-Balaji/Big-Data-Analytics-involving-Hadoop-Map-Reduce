{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Pre-Processinng and WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Monisha\n",
      "[nltk_data]     Balaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Monisha\n",
      "[nltk_data]     Balaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = (\"URLs\\\\topics_tweets.csv\") \n",
    "collected_data_topic = open(\"URLs\\\\Twitter_data.txt\",\"w\")\n",
    "#https://stackoverflow.com/questions/12468179/unicodedecodeerror-utf8-codec-cant-decode-byte-0x9c\n",
    "with open(filename, 'r', encoding='utf-8',errors='ignore') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        try:\n",
    "            collected_data_topic.write(row[5])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_content_topic = open(\"URLs\\\\Twitter_cleaned_data.txt\",\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Cleansing Processes\n",
    "1. Word tokenization\n",
    "2. is aplhabet\n",
    "3. removing emojis\n",
    "4. removing http and https links\n",
    "5. remvong extra space\n",
    "6. stop word removal\n",
    "7. lemmatization\n",
    "8. lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_data_topic = open(\"URLs\\\\Twitter_data.txt\",\"r\")\n",
    "for sentence in collected_data_topic:\n",
    "    RE_EMOJI = re.compile('[^\\x01-\\x7F]', flags=re.UNICODE)\n",
    "    sentence = RE_EMOJI.sub(r'', sentence)\n",
    "    sentence = re.sub(r\"http\\S+\", \" \", sentence)\n",
    "    sentence = re.sub(r\" +\", \" \", sentence)\n",
    "    if not len(sentence.strip()) == 0:\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        for word in word_tokens:\n",
    "            word = re.sub('@[^\\s]+',\" \", word)\n",
    "            if not word in stop_words:\n",
    "                #if d.check(word):\n",
    "                if word.isalpha():\n",
    "                    word_lem = lemmatizer.lemmatize(word.lower())\n",
    "                    #print(word)\n",
    "                    filtered_content_topic.write(word_lem + \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing txt to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   word  frequency\n",
      "rape      NaN NaN NaN NaN NaN NaN   NaN       8799\n",
      "theft     NaN NaN NaN NaN NaN NaN   NaN       8701\n",
      "terrorism NaN NaN NaN NaN NaN NaN   NaN       8513\n",
      "murder    NaN NaN NaN NaN NaN NaN   NaN       7887\n",
      "robbery   NaN NaN NaN NaN NaN NaN   NaN       6643\n",
      "amp       NaN NaN NaN NaN NaN NaN   NaN       3188\n",
      "woman     NaN NaN NaN NaN NaN NaN   NaN       2326\n",
      "police    NaN NaN NaN NaN NaN NaN   NaN       2149\n",
      "you       NaN NaN NaN NaN NaN NaN   NaN       1844\n",
      "and       NaN NaN NaN NaN NaN NaN   NaN       1484\n"
     ]
    }
   ],
   "source": [
    "filename = open(\"URLs\\\\Twitter_wordcount.txt\", \"r\").readlines()\n",
    "\n",
    "import pandas as pd \n",
    "import csv\n",
    "with open('URLs\\\\Twitter_wordcount_csv_1.csv', 'w') as outfile:\n",
    "    mywriter = csv.writer(outfile)\n",
    "    mywriter.writerow(('word', 'frequency'))\n",
    "    for lines in filename:\n",
    "        if not len(lines) == 0:\n",
    "            sentence = lines.rstrip().split(\" \")\n",
    "            mywriter.writerow(sentence)\n",
    "            #print(sentence)\n",
    "\n",
    "data = pd.read_csv(\"URLs\\\\Twitter_wordcount_csv_1.csv\") \n",
    "data.sort_values('frequency', inplace=True, ascending=False)\n",
    "\n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     word  frequency\n",
      "44549          auto theft       1142\n",
      "187018        grand theft       1131\n",
      "35045       armed robbery        926\n",
      "205892     identity theft        714\n",
      "270150        or portland        608\n",
      "50483        bank robbery        536\n",
      "311185     taxation theft        496\n",
      "119483         dame notre        493\n",
      "219313  islamic terrorism        474\n",
      "295633    robbery suspect        441\n"
     ]
    }
   ],
   "source": [
    "filename = open(\"URLs\\\\Twitter_wordco.txt\", \"r\").readlines()\n",
    "\n",
    "import pandas as pd \n",
    "import csv\n",
    "with open('URLs\\\\Twitter_wordco_csv_1.csv', 'w') as outfile:\n",
    "    mywriter = csv.writer(outfile)\n",
    "    mywriter.writerow(('word', 'frequency'))\n",
    "    for lines in filename:\n",
    "        if not len(lines) == 0:\n",
    "            sentence = lines.rstrip().split(\"\\t\")\n",
    "            mywriter.writerow(sentence)\n",
    "            #print(sentence)\n",
    "\n",
    "data = pd.read_csv(\"URLs\\\\Twitter_wordco_csv_1.csv\") \n",
    "data.sort_values('frequency', inplace=True, ascending=False)\n",
    "\n",
    "\n",
    "print(data[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
